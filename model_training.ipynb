{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af99aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9ec73",
   "metadata": {},
   "source": [
    "<H1>Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ea6cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data 20000\n"
     ]
    }
   ],
   "source": [
    "#Baca Hasil Scraping\n",
    "df = pd.read_csv('shopee_reviews.csv')\n",
    "df.dropna(subset=['content'], inplace=True)\n",
    "\n",
    "print('Jumlah data', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c77c49",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Sengaja saat scraping hingga 20ribu data agar sesuai dengan penilaian yang minimal 10ribu sample data setelah menghapus beberapa data duplikat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b313879",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dbe2c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gerald Yoseka</td>\n",
       "      <td>bagus</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:21:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yuliana Ziraluo</td>\n",
       "      <td>bagus sekali untuk mencari barang yang dibutuhkan</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:21:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wahyu Qur'an</td>\n",
       "      <td>sebagai member platinum shopee kecewa rasanya ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-08 13:21:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aat Atmanah</td>\n",
       "      <td>kadang kesel kalau tokonya yang ngirim sesuai ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:19:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander Darren</td>\n",
       "      <td>belanja enak,,banyak diskon</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:19:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userName                                            content  score  \\\n",
       "0     Gerald Yoseka                                              bagus      5   \n",
       "1   Yuliana Ziraluo  bagus sekali untuk mencari barang yang dibutuhkan      5   \n",
       "2      Wahyu Qur'an  sebagai member platinum shopee kecewa rasanya ...      1   \n",
       "3       Aat Atmanah  kadang kesel kalau tokonya yang ngirim sesuai ...      5   \n",
       "4  Alexander Darren                        belanja enak,,banyak diskon      5   \n",
       "\n",
       "                    at  \n",
       "0  2025-04-08 13:21:45  \n",
       "1  2025-04-08 13:21:38  \n",
       "2  2025-04-08 13:21:33  \n",
       "3  2025-04-08 13:19:40  \n",
       "4  2025-04-08 13:19:36  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80615542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   userName  20000 non-null  object\n",
      " 1   content   20000 non-null  object\n",
      " 2   score     20000 non-null  int64 \n",
      " 3   at        20000 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366feadf",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Tidak ada missing values pada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9677fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah duplicate data: 5254\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = df.duplicated(subset=['content']).sum()\n",
    "print(f'Jumlah duplicate data: {duplicate_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6d206",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- ada beberapa duplicate data yang perlu dihapus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8fc40e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data setelah hapus duplicate: 14746\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.drop_duplicates(subset=['content']).reset_index(drop=True)\n",
    "\n",
    "print(f'Jumlah data setelah hapus duplicate: {len(df_clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67d1fd",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Setelah melakukan penghapusan data, maka data yang dapat saya gunakan ada 14746 data. Sehingga sesuai dengan penilaian yang ada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58120f5",
   "metadata": {},
   "source": [
    "<h1> Text Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9fadde42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d0b78",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Import beberapa library yang dibutuhkan saat text processing saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ced74073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Cleaning: Membersihkan teks dari hal-hal tidak perlu\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text) # Menghapus link\n",
    "    text = re.sub(r\"\\d+\", '', text) # Menghapus angka\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) # Menghapus simbol\n",
    "    text = text.replace('\\n', ' ') # Mengganti newline dengan spasi\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#2. Konversi ke huruf kecil\n",
    "def casefoldingText(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#3. Tokenisasi\n",
    "def tokenizingText(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "#4. Menghapus stopwords (Indonesia + Inggris + tambahan kata umum)\n",
    "def filteringText(text):\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update([\n",
    "        'iya','yaa','gak','nya','na','sih','ku',\n",
    "        'di','ga','ya','gaa','loh','kah','woi','woii','woy',\n",
    "        'barang', 'belanja',\"checkout\",\"co\", \"yg\", \"yang\", \"udah\",\n",
    "        'kalo', 'kalau',\"pokoknya\", \"tolong\", \"online\",\n",
    "        'spx', 'beli', 'bintang', 'pesan', 'aplikasi', 'apk'\n",
    "    ])\n",
    "    return [word for word in text if word not in listStopwords]\n",
    "\n",
    "#5. Stemming (Bahasa Indonesia)\n",
    "def stemmingText(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "#6. Ubah list token jadi kalimat lagi\n",
    "def toSentence(list_words):\n",
    "    return ' '.join(list_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42a2f0",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Membuat beberapa fungsi untuk text processing diantaranya :\n",
    "    - cleaning text\n",
    "    - lowercase text (case folding)\n",
    "    - tokenisasi (Memisahkan kalimat menjadi beberapa text tersendiri)\n",
    "    - stopwords untuk menghapus beberapa kata yang tidak dibutuhkan\n",
    "    - stemming\n",
    "    - list words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ef01b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {\n",
    "    \"bgt\": \"banget\", \"maks\": \"maksimal\", \"udh\": \"sudah\", \"aja\": \"saja\", \"blm\": \"belum\", \n",
    "    \"trs\": \"terus\", \"tdk\": \"tidak\", \"gk\": \"tidak\", \"ga\": \"tidak\", \"dr\": \"dari\", \"dpt\": \"dapat\",\n",
    "    \"brg\": \"barang\", \"bgus\": \"bagus\", \"bangus\": \"bagus\", \"gpp\": \"tidak apa\", \"sgt\": \"sangat\", \"bnget\" : \"banget\", \"bngtt\" : \"banget\", \"baguss\" : \"bagus\",\n",
    "    \"bgtt\" : \"banget\", \"bgttt\" : \"banget\", \"bgtttt\" : \"banget\", \"bgttttt\" : \"banget\", \"bgtttttt\" : \"banget\", \"bgttttttt\" : \"banget\",\n",
    "    \"jlk\": \"jelek\", \"jlkk\": \"jelek\", \"jlkkk\": \"jelek\", \"jlkkkk\": \"jelek\", \"jlkkkkk\": \"jelek\", \"jlkkkkkk\": \"jelek\", \"jlkkkkkkk\": \"jelek\",\n",
    "    \"lmot\": \"lemot\", \"lmott\": \"lemot\", \"lmottt\": \"lemot\", \"lmotttt\": \"lemot\", \"lmottttt\": \"lemot\", \"lmotttttt\": \"lemot\", \"lmottttttt\": \"lemot\",\n",
    "    \"lmotttttttt\": \"lemot\", \"lmottttttttt\": \"lemot\", \"lmotttttttttt\": \"lemot\", \"lmottttttttttt\": \"lemot\", \"lmotttttttttttt\": \"lemot\",\n",
    "    \"blnj\": \"belanja\", \"blnjn\": \"belanja\", \"blnjnn\": \"belanja\", \"blnjnnn\": \"belanja\", \"blnjnnnn\": \"belanja\", \"blnjnnnnn\": \"belanja\",\n",
    "    \"bngytt\": \"banget\" , \"sukaa\": \"suka\", \"suka\": \"suka\", \"sukaaa\": \"suka\", \"sukaaaa\": \"suka\", \"sukaaaaa\": \"suka\", \"sukaaaaaa\": \"suka\", \n",
    "    \"ggl\": \"gagal\", \"gagl\" : \"gagal\", \"gagl\": \"gagal\",\n",
    "}\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843c4ed",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Membuat slangwords untuk mengganti beberapa kata singkat singkat atau gaul agar bisa menjadi satu kata baku yang bisa di gunakan saat melabelkan nanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a7059d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "df_clean['text_clean'] = df_clean['content'].apply(clean_text)\n",
    "\n",
    "#Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "df_clean['text_casefoldingText'] = df_clean['text_clean'].apply(casefoldingText)\n",
    "\n",
    "#Mengganti kata - kata slang dengan kata - kata standar dan menyimpannya di 'text_slangwords'\n",
    "df_clean['text_slangwords'] = df_clean['text_casefoldingText'].apply(fix_slangwords)\n",
    "\n",
    "#Memecah teks menjadi token (kata - kata) dan menyimpannya di 'text_tokenizingText'\n",
    "df_clean['text_tokenizingText'] = df_clean['text_slangwords'].apply(tokenizingText)\n",
    "\n",
    "#Menghapus kata - kata stop (kata kata umum) dan menyimpannya di 'text_stopwords'\n",
    "df_clean['text_stopwords'] = df_clean['text_tokenizingText'].apply(filteringText)\n",
    "\n",
    "#Menghubungkan token - token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "df_clean['text_akhir'] = df_clean['text_stopwords'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415320fc",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Mengimplementasi semua fungsi yang sudah dibuat tadi ke dalam df_clean dan membuat kolom baru pada table df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad234747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>at</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopwords</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gerald Yoseka</td>\n",
       "      <td>bagus</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:21:45</td>\n",
       "      <td>bagus</td>\n",
       "      <td>bagus</td>\n",
       "      <td>bagus</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>bagus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yuliana Ziraluo</td>\n",
       "      <td>bagus sekali untuk mencari barang yang dibutuhkan</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:21:38</td>\n",
       "      <td>bagus sekali untuk mencari barang yang dibutuhkan</td>\n",
       "      <td>bagus sekali untuk mencari barang yang dibutuhkan</td>\n",
       "      <td>bagus sekali untuk mencari barang yang dibutuhkan</td>\n",
       "      <td>[bagus, sekali, untuk, mencari, barang, yang, ...</td>\n",
       "      <td>[bagus, mencari, dibutuhkan]</td>\n",
       "      <td>bagus mencari dibutuhkan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wahyu Qur'an</td>\n",
       "      <td>sebagai member platinum shopee kecewa rasanya ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-04-08 13:21:33</td>\n",
       "      <td>sebagai member platinum shopee kecewa rasanya ...</td>\n",
       "      <td>sebagai member platinum shopee kecewa rasanya ...</td>\n",
       "      <td>sebagai member platinum shopee kecewa rasanya ...</td>\n",
       "      <td>[sebagai, member, platinum, shopee, kecewa, ra...</td>\n",
       "      <td>[member, platinum, shopee, kecewa, jnt, hapus,...</td>\n",
       "      <td>member platinum shopee kecewa jnt hapus karna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aat Atmanah</td>\n",
       "      <td>kadang kesel kalau tokonya yang ngirim sesuai ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:19:40</td>\n",
       "      <td>kadang kesel kalau tokonya yang ngirim sesuai ...</td>\n",
       "      <td>kadang kesel kalau tokonya yang ngirim sesuai ...</td>\n",
       "      <td>kadang kesel kalau tokonya yang ngirim sesuai ...</td>\n",
       "      <td>[kadang, kesel, kalau, tokonya, yang, ngirim, ...</td>\n",
       "      <td>[kadang, kesel, tokonya, ngirim, sesuai, pesen...</td>\n",
       "      <td>kadang kesel tokonya ngirim sesuai pesenansopi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander Darren</td>\n",
       "      <td>belanja enak,,banyak diskon</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-04-08 13:19:36</td>\n",
       "      <td>belanja enakbanyak diskon</td>\n",
       "      <td>belanja enakbanyak diskon</td>\n",
       "      <td>belanja enakbanyak diskon</td>\n",
       "      <td>[belanja, enakbanyak, diskon]</td>\n",
       "      <td>[enakbanyak, diskon]</td>\n",
       "      <td>enakbanyak diskon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>elmayusma neli</td>\n",
       "      <td>enak bnget dpt koin cuman modal rebahan</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-25 12:44:00</td>\n",
       "      <td>enak bnget dpt koin cuman modal rebahan</td>\n",
       "      <td>enak bnget dpt koin cuman modal rebahan</td>\n",
       "      <td>enak banget dapat koin cuman modal rebahan</td>\n",
       "      <td>[enak, banget, dapat, koin, cuman, modal, reba...</td>\n",
       "      <td>[enak, banget, koin, cuman, modal, rebahan]</td>\n",
       "      <td>enak banget koin cuman modal rebahan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>Rasya Rasya</td>\n",
       "      <td>sangat cocok untuk belanja online dengan harga...</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-25 12:43:01</td>\n",
       "      <td>sangat cocok untuk belanja online dengan harga...</td>\n",
       "      <td>sangat cocok untuk belanja online dengan harga...</td>\n",
       "      <td>sangat cocok untuk belanja online dengan harga...</td>\n",
       "      <td>[sangat, cocok, untuk, belanja, online, dengan...</td>\n",
       "      <td>[cocok, harga, terjangkau]</td>\n",
       "      <td>cocok harga terjangkau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14743</th>\n",
       "      <td>Grisella</td>\n",
       "      <td>SUMPAH BAGUSS BNGYTT MUDAH BUAT AKU YANG KAUM ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-25 12:42:32</td>\n",
       "      <td>SUMPAH BAGUSS BNGYTT MUDAH BUAT AKU YANG KAUM ...</td>\n",
       "      <td>sumpah baguss bngytt mudah buat aku yang kaum ...</td>\n",
       "      <td>sumpah bagus banget mudah buat aku yang kaum m...</td>\n",
       "      <td>[sumpah, bagus, banget, mudah, buat, aku, yang...</td>\n",
       "      <td>[sumpah, bagus, banget, mudah, kaum, mageransu...</td>\n",
       "      <td>sumpah bagus banget mudah kaum mageransuskes s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14744</th>\n",
       "      <td>Nurhayati Guni</td>\n",
       "      <td>bangus bngtt aplikasi shopee inii😍😍</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-25 12:42:29</td>\n",
       "      <td>bangus bngtt aplikasi shopee inii</td>\n",
       "      <td>bangus bngtt aplikasi shopee inii</td>\n",
       "      <td>bagus banget aplikasi shopee inii</td>\n",
       "      <td>[bagus, banget, aplikasi, shopee, inii]</td>\n",
       "      <td>[bagus, banget, shopee, inii]</td>\n",
       "      <td>bagus banget shopee inii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14745</th>\n",
       "      <td>sarino rino</td>\n",
       "      <td>barang nya murah murah bagus</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-25 12:41:46</td>\n",
       "      <td>barang nya murah murah bagus</td>\n",
       "      <td>barang nya murah murah bagus</td>\n",
       "      <td>barang nya murah murah bagus</td>\n",
       "      <td>[barang, nya, murah, murah, bagus]</td>\n",
       "      <td>[murah, murah, bagus]</td>\n",
       "      <td>murah murah bagus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14746 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               userName                                            content  \\\n",
       "0         Gerald Yoseka                                              bagus   \n",
       "1       Yuliana Ziraluo  bagus sekali untuk mencari barang yang dibutuhkan   \n",
       "2          Wahyu Qur'an  sebagai member platinum shopee kecewa rasanya ...   \n",
       "3           Aat Atmanah  kadang kesel kalau tokonya yang ngirim sesuai ...   \n",
       "4      Alexander Darren                        belanja enak,,banyak diskon   \n",
       "...                 ...                                                ...   \n",
       "14741    elmayusma neli            enak bnget dpt koin cuman modal rebahan   \n",
       "14742       Rasya Rasya  sangat cocok untuk belanja online dengan harga...   \n",
       "14743          Grisella  SUMPAH BAGUSS BNGYTT MUDAH BUAT AKU YANG KAUM ...   \n",
       "14744    Nurhayati Guni                bangus bngtt aplikasi shopee inii😍😍   \n",
       "14745       sarino rino                       barang nya murah murah bagus   \n",
       "\n",
       "       score                   at  \\\n",
       "0          5  2025-04-08 13:21:45   \n",
       "1          5  2025-04-08 13:21:38   \n",
       "2          1  2025-04-08 13:21:33   \n",
       "3          5  2025-04-08 13:19:40   \n",
       "4          5  2025-04-08 13:19:36   \n",
       "...      ...                  ...   \n",
       "14741      5  2025-03-25 12:44:00   \n",
       "14742      5  2025-03-25 12:43:01   \n",
       "14743      5  2025-03-25 12:42:32   \n",
       "14744      5  2025-03-25 12:42:29   \n",
       "14745      5  2025-03-25 12:41:46   \n",
       "\n",
       "                                              text_clean  \\\n",
       "0                                                  bagus   \n",
       "1      bagus sekali untuk mencari barang yang dibutuhkan   \n",
       "2      sebagai member platinum shopee kecewa rasanya ...   \n",
       "3      kadang kesel kalau tokonya yang ngirim sesuai ...   \n",
       "4                              belanja enakbanyak diskon   \n",
       "...                                                  ...   \n",
       "14741            enak bnget dpt koin cuman modal rebahan   \n",
       "14742  sangat cocok untuk belanja online dengan harga...   \n",
       "14743  SUMPAH BAGUSS BNGYTT MUDAH BUAT AKU YANG KAUM ...   \n",
       "14744                  bangus bngtt aplikasi shopee inii   \n",
       "14745                       barang nya murah murah bagus   \n",
       "\n",
       "                                    text_casefoldingText  \\\n",
       "0                                                  bagus   \n",
       "1      bagus sekali untuk mencari barang yang dibutuhkan   \n",
       "2      sebagai member platinum shopee kecewa rasanya ...   \n",
       "3      kadang kesel kalau tokonya yang ngirim sesuai ...   \n",
       "4                              belanja enakbanyak diskon   \n",
       "...                                                  ...   \n",
       "14741            enak bnget dpt koin cuman modal rebahan   \n",
       "14742  sangat cocok untuk belanja online dengan harga...   \n",
       "14743  sumpah baguss bngytt mudah buat aku yang kaum ...   \n",
       "14744                  bangus bngtt aplikasi shopee inii   \n",
       "14745                       barang nya murah murah bagus   \n",
       "\n",
       "                                         text_slangwords  \\\n",
       "0                                                  bagus   \n",
       "1      bagus sekali untuk mencari barang yang dibutuhkan   \n",
       "2      sebagai member platinum shopee kecewa rasanya ...   \n",
       "3      kadang kesel kalau tokonya yang ngirim sesuai ...   \n",
       "4                              belanja enakbanyak diskon   \n",
       "...                                                  ...   \n",
       "14741         enak banget dapat koin cuman modal rebahan   \n",
       "14742  sangat cocok untuk belanja online dengan harga...   \n",
       "14743  sumpah bagus banget mudah buat aku yang kaum m...   \n",
       "14744                  bagus banget aplikasi shopee inii   \n",
       "14745                       barang nya murah murah bagus   \n",
       "\n",
       "                                     text_tokenizingText  \\\n",
       "0                                                [bagus]   \n",
       "1      [bagus, sekali, untuk, mencari, barang, yang, ...   \n",
       "2      [sebagai, member, platinum, shopee, kecewa, ra...   \n",
       "3      [kadang, kesel, kalau, tokonya, yang, ngirim, ...   \n",
       "4                          [belanja, enakbanyak, diskon]   \n",
       "...                                                  ...   \n",
       "14741  [enak, banget, dapat, koin, cuman, modal, reba...   \n",
       "14742  [sangat, cocok, untuk, belanja, online, dengan...   \n",
       "14743  [sumpah, bagus, banget, mudah, buat, aku, yang...   \n",
       "14744            [bagus, banget, aplikasi, shopee, inii]   \n",
       "14745                 [barang, nya, murah, murah, bagus]   \n",
       "\n",
       "                                          text_stopwords  \\\n",
       "0                                                [bagus]   \n",
       "1                           [bagus, mencari, dibutuhkan]   \n",
       "2      [member, platinum, shopee, kecewa, jnt, hapus,...   \n",
       "3      [kadang, kesel, tokonya, ngirim, sesuai, pesen...   \n",
       "4                                   [enakbanyak, diskon]   \n",
       "...                                                  ...   \n",
       "14741        [enak, banget, koin, cuman, modal, rebahan]   \n",
       "14742                         [cocok, harga, terjangkau]   \n",
       "14743  [sumpah, bagus, banget, mudah, kaum, mageransu...   \n",
       "14744                      [bagus, banget, shopee, inii]   \n",
       "14745                              [murah, murah, bagus]   \n",
       "\n",
       "                                              text_akhir  \n",
       "0                                                  bagus  \n",
       "1                               bagus mencari dibutuhkan  \n",
       "2      member platinum shopee kecewa jnt hapus karna ...  \n",
       "3      kadang kesel tokonya ngirim sesuai pesenansopi...  \n",
       "4                                      enakbanyak diskon  \n",
       "...                                                  ...  \n",
       "14741               enak banget koin cuman modal rebahan  \n",
       "14742                             cocok harga terjangkau  \n",
       "14743  sumpah bagus banget mudah kaum mageransuskes s...  \n",
       "14744                           bagus banget shopee inii  \n",
       "14745                                  murah murah bagus  \n",
       "\n",
       "[14746 rows x 10 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea3af5",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Berikut diatas hasil dari penambahan kolom dari fungsi fungsi yang sudah dibuat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e1d74",
   "metadata": {},
   "source": [
    "<h1> Pelabelan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "12b25218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Inisialisasi dictionary kosong untuk menyimpan kata-kata positif\n",
    "lexicon_positive = dict()\n",
    "\n",
    "# Mengirim request untuk mengambil data CSV dari GitHub (kamus kata positif)\n",
    "response_pos = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "\n",
    "# Mengecek apakah request berhasil\n",
    "if response_pos.status_code == 200:\n",
    "    # Membaca isi CSV sebagai list baris, dipisahkan berdasarkan koma\n",
    "    reader = csv.reader(StringIO(response_pos.text), delimiter=',')\n",
    "    \n",
    "    # Menyimpan setiap kata positif dan skornya ke dalam dictionary\n",
    "    for row in reader:\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "else:\n",
    "    # Menampilkan pesan error jika file gagal diambil\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    "\n",
    "\n",
    "# Inisialisasi dictionary kosong untuk menyimpan kata-kata negatif\n",
    "lexicon_negative = dict()\n",
    "\n",
    "# Mengirim request untuk mengambil data CSV dari GitHub (kamus kata negatif)\n",
    "response_neg = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "\n",
    "# Mengecek apakah request berhasil\n",
    "if response_neg.status_code == 200:\n",
    "    # Membaca isi CSV sebagai list baris, dipisahkan berdasarkan koma\n",
    "    reader = csv.reader(StringIO(response_neg.text), delimiter=',')\n",
    "    \n",
    "    # Menyimpan setiap kata negatif dan skornya ke dalam dictionary\n",
    "    for row in reader:\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "else:\n",
    "    # Menampilkan pesan error jika file gagal diambil\n",
    "    print(\"Failed to fetch negative lexicon data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b865d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Positif: 2\n",
      "Negatif: -4\n"
     ]
    }
   ],
   "source": [
    "print(\"bagus\" in lexicon_positive)\n",
    "print(\"bagus\" in lexicon_negative)\n",
    "\n",
    "if \"bagus\" in lexicon_positive:\n",
    "    print(\"Positif:\", lexicon_positive[\"bagus\"])\n",
    "if \"bagus\" in lexicon_negative:\n",
    "    print(\"Negatif:\", lexicon_negative[\"bagus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95614c1",
   "metadata": {},
   "source": [
    "**Insight** \n",
    "- Sebelum melakukan kode pengecekan kata 'bagus' pada lexicon, saya sudah melakukan pengecekan top words negative dan paling tinggi ialah bagus sehingga tidak normal hasilnya. dan ternyata pada negative nilainya -4 sehingga perlu dilakukan penghapusan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0daa851",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bagus\" in lexicon_negative:\n",
    "    del lexicon_negative[\"bagus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b7b82",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Menghapus kata bagus pada lexicon negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11da686",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Mengambil 2 csv dari github yang berisi kata kata positif dan negatif serta skor dikeduanya yang kemudian dihubungkan kedalam dictionary kosong positif dan negatif dengan nama lexicon_positive dan juga lexicon_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2db6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menentukan polaritas sentimen dari sebuah teks menggunakan pendekatan lexicon-based\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    # Inisialisasi skor awal\n",
    "    score = 0\n",
    "\n",
    "    # Menambahkan skor positif jika kata ditemukan dalam lexicon positif\n",
    "    for word in text:\n",
    "        if word in lexicon_positive:\n",
    "            score = score + lexicon_positive[word]\n",
    "\n",
    "    # Menambahkan skor negatif jika kata ditemukan dalam lexicon negatif\n",
    "    for word in text:\n",
    "        if word in lexicon_negative:\n",
    "            score = score + lexicon_negative[word]\n",
    "\n",
    "    # Menentukan label polaritas berdasarkan nilai skor akhir\n",
    "    polarity = ''\n",
    "    if score >= 0:\n",
    "        polarity = 'positive'  # Jika skor 0 atau lebih, dianggap positif\n",
    "    elif score < 0:\n",
    "        polarity = 'negative'  # Jika skor kurang dari 0, dianggap negatif\n",
    "\n",
    "    # Mengembalikan skor sentimen dan polaritasnya\n",
    "    return score, polarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ad20e",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Membuat fungsi untuk menentukan polaritas sentimen dari text dengan pendekatan lexicon-based dimana diberi fungsi jika menemukan kata positif maka masuk ke dalam lexicon positif jika negative maka masuk kedalam lexicon negative. dan jika nilai score akhir lebih dari sama dengan 0 maka positif, jika kurang dari 0 maka negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6140321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "positive    11419\n",
      "negative     3327\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Menerapkan fungsi analisis sentimen ke setiap baris pada kolom 'text_stopwords'\n",
    "results = df_clean['text_stopwords'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "\n",
    "# Mengubah hasil menjadi dua list: satu untuk skor, satu untuk label polaritas\n",
    "results = list(zip(*results))\n",
    "\n",
    "# Menyimpan skor polaritas ke kolom baru 'polarity_score'\n",
    "df_clean['polarity_score'] = results[0]\n",
    "\n",
    "# Menyimpan label sentimen (positive/negative) ke kolom 'polarity'\n",
    "df_clean['polarity'] = results[1]\n",
    "\n",
    "# Menampilkan jumlah data untuk setiap label sentimen\n",
    "print(df_clean['polarity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb9eb4",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Menerapkan fungsi analisis sentimen berbasis lexicon (sentiment_analysis_lexicon_indonesia) pada teks yang telah dibersihkan (text_stopwords).\n",
    "- Menghasilkan:\n",
    "    - Skor sentimen numerik (polarity_score)\n",
    "    - Label kategori sentimen (polarity: seperti 'positive', 'negative', dst)\n",
    "- Menyimpan hasilnya ke dalam DataFrame (df_clean)\n",
    "- Menampilkan jumlah data untuk tiap label sentimen → untuk mengecek distribusi kelas.\n",
    "- Hasil didapatkan positive 9854, dan negative 4892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "33824dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_clean['text_akhir']\n",
    "y = df_clean['polarity']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8)\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "features_df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e34822",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Membuat representasi fitur dari teks menggunakan TF-IDF\n",
    "- Mengontrol fitur yang digunakan dengan parameter filtering\n",
    "- Split data menjadi data latih dan data uji (80%:20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf4900d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words untuk label 'negative':\n",
      "pengiriman    0.075622\n",
      "shopee        0.046627\n",
      "banget        0.023598\n",
      "cepat         0.019472\n",
      "lemot         0.019419\n",
      "kurir         0.017504\n",
      "buruk         0.016357\n",
      "jelek         0.015978\n",
      "bagus         0.015962\n",
      "susah         0.015397\n",
      "ribet         0.014704\n",
      "pesanan       0.013902\n",
      "jt            0.012601\n",
      "akun          0.012546\n",
      "iklan         0.012375\n",
      "Name: negative, dtype: float64\n",
      "\n",
      "Top words untuk label 'positive':\n",
      "shopee        0.071492\n",
      "bagus         0.068116\n",
      "membantu      0.033025\n",
      "mantap        0.030032\n",
      "banget        0.025950\n",
      "mudah         0.025792\n",
      "suka          0.024106\n",
      "puas          0.023748\n",
      "sesuai        0.023410\n",
      "murah         0.021243\n",
      "cepat         0.019641\n",
      "ongkir        0.016014\n",
      "berbelanja    0.015387\n",
      "pelayanan     0.014257\n",
      "memuaskan     0.014079\n",
      "Name: positive, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Asumsikan 'text_akhir' udah bersih & 'polarity' adalah label sentimen\n",
    "texts = df_clean['text_akhir']\n",
    "labels = df_clean['polarity']\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words=None)\n",
    "X_tfidf = tfidf.fit_transform(texts)\n",
    "\n",
    "# Buat DataFrame TF-IDF\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "tfidf_df['label'] = labels.values\n",
    "\n",
    "# Rata-rata skor TF-IDF per label\n",
    "mean_tfidf = tfidf_df.groupby('label').mean().T\n",
    "\n",
    "# Tampilkan 15 kata paling informatif per kelas\n",
    "for label in mean_tfidf.columns:\n",
    "    print(f\"\\nTop words untuk label '{label}':\")\n",
    "    print(mean_tfidf[label].sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0ab13",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Menggunakan TF-IDF untuk merepresentasikan teks ke bentuk numerik.\n",
    "- Menghitung rata-rata skor TF-IDF untuk tiap kata berdasarkan label (positive, negative, [neutral]).\n",
    "- Menampilkan top words atau kata paling representatif untuk tiap sentimen. \n",
    "- Kita bisa melihat beberapa kata yang berpengaruh pada positif dan negative, sehingga saat ada kata kata yang tidak penting dan masuk ke dalam kategori tersebut perlu di hilangkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0ec6cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - accuracy_train:  0.8381654798236691\n",
      "Naive Bayes - accuracy_test:  0.8386440677966102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Membuat objek naive bayes\n",
    "naive_bayes = BernoulliNB()\n",
    "\n",
    "#Melatih model naive bayes pada data pelatihan\n",
    "naive_bayes.fit(X_train.toarray(), y_train)\n",
    "\n",
    "#Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_nb = naive_bayes.predict(X_train.toarray())\n",
    "y_pred_test_nb = naive_bayes.predict(X_test.toarray())\n",
    "\n",
    "#Evaluasi akurasi model Naive Bayes\n",
    "accuracy_train_nb = accuracy_score(y_pred_train_nb, y_train)\n",
    "accuracy_test_nb = accuracy_score(y_pred_test_nb, y_test)\n",
    "\n",
    "#Menampilkan akurasi\n",
    "print('Naive Bayes - accuracy_train: ', accuracy_train_nb)\n",
    "print('Naive Bayes - accuracy_test: ', accuracy_test_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1fa1e4",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Menggunakan varian BernoulliNB pada naive bayes menghasilkan 0.81 pada akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "482e6e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB - accuracy_train: 0.8683452017633096\n",
      "MultinomialNB - accuracy_test: 0.868135593220339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Membuat objek model Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "\n",
    "# Melatih model pada data training\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi pada data training dan testing\n",
    "y_pred_train_mnb = multinomial_nb.predict(X_train)\n",
    "y_pred_test_mnb = multinomial_nb.predict(X_test)\n",
    "\n",
    "# Evaluasi akurasi model\n",
    "accuracy_train_mnb = accuracy_score(y_train, y_pred_train_mnb)\n",
    "accuracy_test_mnb = accuracy_score(y_test, y_pred_test_mnb)\n",
    "\n",
    "# Menampilkan hasil akurasi\n",
    "print(\"MultinomialNB - accuracy_train:\", accuracy_train_mnb)\n",
    "print(\"MultinomialNB - accuracy_test:\", accuracy_test_mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9d7bb",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Sudah mencoba mengubah bebera text processing namun yang dihasilkan tadi mentok 0.81 sehingga saya mencoba menggunakan varian lain yang lebih cocok untuk sentimen pada naive bayes yaitu MultinomialNB mendapatkan akurasi 0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2726e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Train Accuracy: 0.9326\n",
      "Logistic Regression - Test Accuracy : 0.9136\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.66      0.77       639\n",
      "    positive       0.91      0.98      0.95      2311\n",
      "\n",
      "    accuracy                           0.91      2950\n",
      "   macro avg       0.92      0.82      0.86      2950\n",
      "weighted avg       0.91      0.91      0.91      2950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Data\n",
    "X = df_clean['text_akhir']  # kolom teks hasil preprocessing akhir\n",
    "y = df_clean['polarity']    # label sentimen: 'positive', 'negative', (atau 'neutral')\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.9)\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluasi\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Logistic Regression - Train Accuracy:\", round(acc_train, 4))\n",
    "print(\"Logistic Regression - Test Accuracy :\", round(acc_test, 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c20f08",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Alhamdulillah menggunakan logistic jadi 0.93 tertingginya untuk akurasi train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1cbec",
   "metadata": {},
   "source": [
    "<h1>Simpan Model dan Vectorizer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18006801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Simpan model Logistic Regression\n",
    "joblib.dump(logreg, 'model_sentimen_logistic.pkl')\n",
    "\n",
    "# Simpan TF-IDF vectorizer\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1d560",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3a991cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks: aplikasi ini sangat membantu dan bagus sekali\n",
      "Hasil Analisis Sentimen: POSITIVE\n",
      "Confidence: 99.10%\n"
     ]
    }
   ],
   "source": [
    "# Load model dan vectorizer\n",
    "logistic_model = joblib.load('model_sentimen_logistic.pkl')\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# GANTI teks ini untuk testing\n",
    "user_input = \"aplikasi ini sangat membantu dan bagus sekali\"\n",
    "\n",
    "# Transformasi input ke TF-IDF\n",
    "input_vector = tfidf_vectorizer.transform([user_input])\n",
    "\n",
    "# Prediksi sentimen\n",
    "predicted_label = logistic_model.predict(input_vector)[0]\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(f\"Teks: {user_input}\")\n",
    "print(f\"Hasil Analisis Sentimen: {predicted_label.upper()}\")\n",
    "\n",
    "# (Opsional) Confidence Score\n",
    "proba = logistic_model.predict_proba(input_vector)\n",
    "print(f\"Confidence: {max(proba[0]) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
